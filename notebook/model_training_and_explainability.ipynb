{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Explainability Analysis\n",
    "## Tasks 2 & 3: Model Building, Training, and SHAP Explainability\n",
    "\n",
    "This notebook implements:\n",
    "- **Task 2**: Model building and training (Logistic Regression + Ensemble models)\n",
    "- **Task 3**: Model explainability using SHAP analysis\n",
    "\n",
    "### Objectives\n",
    "1. Train Logistic Regression (baseline) and ensemble models\n",
    "2. Evaluate models using appropriate metrics for imbalanced data\n",
    "3. Select best performing model with justification\n",
    "4. Apply SHAP explainability to understand fraud drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom utilities\n",
    "from utils.data_utils import DataLoader, DataCleaner, merge_with_geolocation\n",
    "from utils.feature_engineering import FeatureEngineer, create_all_features\n",
    "from utils.preprocessing import full_preprocessing_pipeline\n",
    "from utils.model_training import ModelTrainer, cross_validate_models\n",
    "from utils.model_evaluation import evaluate_models_comprehensive\n",
    "from utils.model_explainability import explain_best_model\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA PREPARATION ===\n",
      "Fraud data loaded: (151112, 11)\n",
      "Credit card data loaded: (284807, 31)\n",
      "Missing values before cleaning:\n",
      "user_id           0\n",
      "signup_time       0\n",
      "purchase_time     0\n",
      "purchase_value    0\n",
      "device_id         0\n",
      "source            0\n",
      "browser           0\n",
      "sex               0\n",
      "age               0\n",
      "ip_address        0\n",
      "class             0\n",
      "dtype: int64\n",
      "Missing values after cleaning:\n",
      "user_id           0\n",
      "signup_time       0\n",
      "purchase_time     0\n",
      "purchase_value    0\n",
      "device_id         0\n",
      "source            0\n",
      "browser           0\n",
      "sex               0\n",
      "age               0\n",
      "ip_address        0\n",
      "class             0\n",
      "dtype: int64\n",
      "Duplicates removed: 0 rows\n",
      "Shape before: (151112, 11), Shape after: (151112, 11)\n",
      "Starting comprehensive feature engineering...\n",
      "Time features created: hour_of_day, day_of_week, time_period, is_weekend\n",
      "Time since signup calculated (in hours)\n",
      "Transaction features created for users, devices, and IP addresses\n",
      "Purchase value features created\n",
      "Categorical features encoded: ['source', 'browser', 'sex', 'time_period', 'signup_category', 'purchase_category']\n",
      "Feature engineering complete. Final shape: (151112, 56)\n",
      "Fraud dataset prepared: (151112, 56)\n",
      "Credit card dataset available: (284807, 31)\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data (using preprocessed data from previous analysis)\n",
    "print(\"=== DATA PREPARATION ===\")\n",
    "\n",
    "# Load data\n",
    "data_loader = DataLoader(data_path='../data/')\n",
    "fraud_df = data_loader.load_fraud_data()\n",
    "creditcard_df = data_loader.load_creditcard_data()\n",
    "\n",
    "# Clean and engineer features for fraud data\n",
    "if not fraud_df.empty:\n",
    "    cleaner = DataCleaner()\n",
    "    fraud_df_clean = cleaner.handle_missing_values(fraud_df, strategy='drop')\n",
    "    fraud_df_clean = cleaner.remove_duplicates(fraud_df_clean)\n",
    "    fraud_df_clean = cleaner.correct_data_types(fraud_df_clean)\n",
    "    \n",
    "    # Feature engineering\n",
    "    fraud_df_features = create_all_features(fraud_df_clean)\n",
    "    \n",
    "    print(f\"Fraud dataset prepared: {fraud_df_features.shape}\")\n",
    "else:\n",
    "    print(\"Fraud dataset not available\")\n",
    "\n",
    "# Prepare creditcard data if available\n",
    "if not creditcard_df.empty:\n",
    "    print(f\"Credit card dataset available: {creditcard_df.shape}\")\n",
    "else:\n",
    "    print(\"Credit card dataset not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPROCESSING FRAUD DATA ===\n",
      "Starting full preprocessing pipeline...\n",
      "Preparing features for modeling...\n",
      "Excluding columns: ['user_id', 'device_id', 'ip_address', 'signup_time', 'purchase_time']\n",
      "Converting categorical columns to numeric: ['source', 'browser', 'sex', 'time_period', 'signup_category', 'purchase_category']\n",
      "Converting datetime columns to numeric: ['user_first_transaction', 'user_last_transaction']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (151112, 49), indices imply (151112, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== PREPROCESSING FRAUD DATA ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Full preprocessing pipeline\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m fraud_processed \u001b[38;5;241m=\u001b[39m \u001b[43mfull_preprocessing_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfraud_df_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msmote\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstandard\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     11\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFraud data preprocessing completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfraud_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\senta\\OneDrive\\Documents\\Proj\\10 Ac\\fraud-detection-system\\notebook\\..\\utils\\preprocessing.py:324\u001b[0m, in \u001b[0;36mfull_preprocessing_pipeline\u001b[1;34m(df, target_col, sampling_strategy, scaling_method)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting full preprocessing pipeline...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# Prepare features\u001b[39;00m\n\u001b[1;32m--> 324\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_features_for_modeling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# Split data\u001b[39;00m\n\u001b[0;32m    327\u001b[0m splitter \u001b[38;5;241m=\u001b[39m DataSplitter()\n",
      "File \u001b[1;32mc:\\Users\\senta\\OneDrive\\Documents\\Proj\\10 Ac\\fraud-detection-system\\notebook\\..\\utils\\preprocessing.py:294\u001b[0m, in \u001b[0;36mprepare_features_for_modeling\u001b[1;34m(df, target_col)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleImputer\n\u001b[0;32m    293\u001b[0m imp \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 294\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# If any NaNs remain (e.g., columns that were all NaN), replace with 0\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39many()\u001b[38;5;241m.\u001b[39many():\n",
      "File \u001b[1;32mc:\\Users\\senta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:827\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    817\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    818\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    824\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[0;32m    825\u001b[0m         )\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 827\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\senta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\senta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (151112, 49), indices imply (151112, 50)"
     ]
    }
   ],
   "source": [
    "# Preprocessing pipeline for fraud data\n",
    "if not fraud_df_features.empty:\n",
    "    print(\"=== PREPROCESSING FRAUD DATA ===\")\n",
    "    \n",
    "    # Full preprocessing pipeline\n",
    "    fraud_processed = full_preprocessing_pipeline(\n",
    "        fraud_df_features,\n",
    "        target_col='class',\n",
    "        sampling_strategy='smote',\n",
    "        scaling_method='standard'\n",
    "    )\n",
    "    \n",
    "    print(\"Fraud data preprocessing completed!\")\n",
    "    print(f\"Training set: {fraud_processed['X_train'].shape}\")\n",
    "    print(f\"Test set: {fraud_processed['X_test'].shape}\")\n",
    "    print(f\"Features: {len(fraud_processed['feature_names'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models on fraud data\n",
    "if 'fraud_processed' in locals():\n",
    "    print(\"=== TRAINING MODELS ON FRAUD DATA ===\")\n",
    "    \n",
    "    # Initialize model trainer\n",
    "    trainer = ModelTrainer(random_state=42)\n",
    "    \n",
    "    # Train models\n",
    "    fraud_models = trainer.train_all_models(\n",
    "        fraud_processed['X_train'],\n",
    "        fraud_processed['y_train'],\n",
    "        models_to_train=['logistic_regression', 'random_forest', 'xgboost'],\n",
    "        hyperparameter_tuning=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel training completed for fraud data!\")\n",
    "    print(f\"Models trained: {list(fraud_models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model evaluation for fraud data\n",
    "if 'fraud_models' in locals():\n",
    "    print(\"=== COMPREHENSIVE MODEL EVALUATION ===\")\n",
    "    \n",
    "    # Evaluate all models\n",
    "    fraud_comparison, fraud_best_model = evaluate_models_comprehensive(\n",
    "        fraud_models,\n",
    "        fraud_processed['X_train'],\n",
    "        fraud_processed['y_train'],\n",
    "        fraud_processed['X_test'],\n",
    "        fraud_processed['y_test'],\n",
    "        fraud_processed['feature_names']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nBest model for fraud detection: {fraud_best_model}\")\n",
    "    \n",
    "    # Display comparison results\n",
    "    print(\"\\nModel Comparison Results:\")\n",
    "    display(fraud_comparison[['f1_score', 'precision', 'recall', 'pr_auc', 'roc_auc']].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Selection Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed justification for best model selection\n",
    "if 'fraud_comparison' in locals():\n",
    "    print(\"=== MODEL SELECTION JUSTIFICATION ===\")\n",
    "    print(\"\\nFor fraud detection, we prioritize:\")\n",
    "    print(\"1. F1-Score: Balance between precision and recall\")\n",
    "    print(\"2. PR-AUC: Performance on imbalanced data\")\n",
    "    print(\"3. Recall: Catching actual fraud cases\")\n",
    "    print(\"4. Precision: Minimizing false positives\")\n",
    "    \n",
    "    # Rank models by key metrics\n",
    "    key_metrics = ['f1_score', 'pr_auc', 'recall', 'precision']\n",
    "    \n",
    "    print(\"\\nModel Rankings by Key Metrics:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for metric in key_metrics:\n",
    "        ranked = fraud_comparison.sort_values(metric, ascending=False)\n",
    "        print(f\"\\n{metric.upper()}:\")\n",
    "        for i, (model, score) in enumerate(ranked[metric].items(), 1):\n",
    "            print(f\"  {i}. {model:15}: {score:.4f}\")\n",
    "    \n",
    "    # Business impact analysis\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BUSINESS IMPACT ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    best_model_metrics = fraud_comparison.loc[fraud_best_model]\n",
    "    \n",
    "    print(f\"\\nSelected Model: {fraud_best_model.upper()}\")\n",
    "    print(f\"F1-Score: {best_model_metrics['f1_score']:.4f}\")\n",
    "    print(f\"Precision: {best_model_metrics['precision']:.4f} (False Positive Rate: {1-best_model_metrics['precision']:.4f})\")\n",
    "    print(f\"Recall: {best_model_metrics['recall']:.4f} (False Negative Rate: {1-best_model_metrics['recall']:.4f})\")\n",
    "    print(f\"PR-AUC: {best_model_metrics['pr_auc']:.4f}\")\n",
    "    \n",
    "    print(\"\\nBusiness Justification:\")\n",
    "    print(f\"• Balanced performance with F1-Score of {best_model_metrics['f1_score']:.4f}\")\n",
    "    print(f\"• {best_model_metrics['recall']*100:.1f}% of fraud cases detected\")\n",
    "    print(f\"• {(1-best_model_metrics['precision'])*100:.1f}% false positive rate (acceptable for fraud detection)\")\n",
    "    print(f\"• Strong performance on imbalanced data (PR-AUC: {best_model_metrics['pr_auc']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SHAP Explainability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explainability for best model\n",
    "if 'fraud_best_model' in locals():\n",
    "    print(\"=== SHAP EXPLAINABILITY ANALYSIS ===\")\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = fraud_models[fraud_best_model]\n",
    "    \n",
    "    # Select sample indices for detailed explanation\n",
    "    fraud_indices = fraud_processed['y_test'][fraud_processed['y_test'] == 1].index[:3].tolist()\n",
    "    legit_indices = fraud_processed['y_test'][fraud_processed['y_test'] == 0].index[:3].tolist()\n",
    "    sample_indices = fraud_indices + legit_indices\n",
    "    \n",
    "    print(f\"Analyzing {len(sample_indices)} sample predictions...\")\n",
    "    \n",
    "    # Comprehensive SHAP analysis\n",
    "    fraud_insights = explain_best_model(\n",
    "        best_model,\n",
    "        fraud_best_model,\n",
    "        fraud_processed['X_train'],\n",
    "        fraud_processed['X_test'],\n",
    "        fraud_processed['y_test'],\n",
    "        sample_indices\n",
    "    )\n",
    "    \n",
    "    print(\"\\nSHAP analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Credit Card Data Analysis (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREDIT CARD DATA ANALYSIS ===\n",
      "Starting full preprocessing pipeline...\n",
      "Preparing features for modeling...\n",
      "Final feature matrix shape: (284807, 30)\n",
      "Target variable shape: (284807,)\n",
      "Splitting data: 80% train, 20% test\n",
      "Training set shape: (227845, 30)\n",
      "Test set shape: (56962, 30)\n",
      "\n",
      "Training set class distribution:\n",
      "Class Distribution Analysis:\n",
      "Class counts: {0: 227451, 1: 394}\n",
      "Class proportions: {0: 0.9982707542408216, 1: 0.001729245759178389}\n",
      "Imbalance ratio: 577.29\n",
      "Minority class percentage: 0.17%\n",
      "\n",
      "Test set class distribution:\n",
      "Class Distribution Analysis:\n",
      "Class counts: {0: 56864, 1: 98}\n",
      "Class proportions: {0: 0.9982795547909132, 1: 0.0017204452090867595}\n",
      "Imbalance ratio: 580.24\n",
      "Minority class percentage: 0.17%\n",
      "Applying SMOTE oversampling...\n",
      "Original shape: (227845, 30)\n",
      "Resampled shape: (454902, 30)\n",
      "Class Distribution Analysis:\n",
      "Class counts: {0: 227451, 1: 227451}\n",
      "Class proportions: {0: 0.5, 1: 0.5}\n",
      "Imbalance ratio: 1.00\n",
      "Minority class percentage: 50.00%\n",
      "Applying Standard Scaling...\n",
      "Features scaled: 30\n",
      "Preprocessing pipeline completed!\n",
      "Training 3 models...\n",
      "============================================================\n",
      "\n",
      "LOGISTIC REGRESSION\n",
      "----------------------------------------\n",
      "Training Logistic Regression model...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[0;32m     14\u001b[0m cc_trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m cc_models \u001b[38;5;241m=\u001b[39m \u001b[43mcc_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_all_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreditcard_processed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreditcard_processed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels_to_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogistic_regression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandom_forest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxgboost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameter_tuning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Evaluate models\u001b[39;00m\n\u001b[0;32m     23\u001b[0m cc_comparison, cc_best_model \u001b[38;5;241m=\u001b[39m evaluate_models_comprehensive(\n\u001b[0;32m     24\u001b[0m     cc_models,\n\u001b[0;32m     25\u001b[0m     creditcard_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     creditcard_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_names\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     30\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\senta\\OneDrive\\Documents\\Proj\\10 Ac\\fraud-detection-system\\notebook\\..\\utils\\model_training.py:314\u001b[0m, in \u001b[0;36mModelTrainer.train_all_models\u001b[1;34m(self, X_train, y_train, models_to_train, hyperparameter_tuning)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic_regression\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 314\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_logistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameter_tuning\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_forest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    316\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_random_forest(X_train, y_train, hyperparameter_tuning)\n",
      "File \u001b[1;32mc:\\Users\\senta\\OneDrive\\Documents\\Proj\\10 Ac\\fraud-detection-system\\notebook\\..\\utils\\model_training.py:69\u001b[0m, in \u001b[0;36mModelTrainer.train_logistic_regression\u001b[1;34m(self, X_train, y_train, hyperparameter_tuning)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Perform grid search with cross-validation\u001b[39;00m\n\u001b[0;32m     60\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     61\u001b[0m     base_model, \n\u001b[0;32m     62\u001b[0m     param_grid, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     67\u001b[0m )\n\u001b[1;32m---> 69\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Store best parameters\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic_regression\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\senta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\senta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\senta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\senta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    967\u001b[0m         )\n\u001b[0;32m    968\u001b[0m     )\n\u001b[1;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\senta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\senta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\senta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\senta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1799\u001b[0m     ):\n\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process credit card data if available\n",
    "if not creditcard_df.empty:\n",
    "    print(\"=== CREDIT CARD DATA ANALYSIS ===\")\n",
    "    \n",
    "    # Preprocessing for credit card data\n",
    "    creditcard_processed = full_preprocessing_pipeline(\n",
    "        creditcard_df,\n",
    "        target_col='Class',  # Note: Capital 'C' for creditcard data\n",
    "        sampling_strategy='smote',\n",
    "        scaling_method='standard'\n",
    "    )\n",
    "    \n",
    "    # Train models\n",
    "    cc_trainer = ModelTrainer(random_state=42)\n",
    "    cc_models = cc_trainer.train_all_models(\n",
    "        creditcard_processed['X_train'],\n",
    "        creditcard_processed['y_train'],\n",
    "        models_to_train=['logistic_regression', 'random_forest', 'xgboost'],\n",
    "        hyperparameter_tuning=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate models\n",
    "    cc_comparison, cc_best_model = evaluate_models_comprehensive(\n",
    "        cc_models,\n",
    "        creditcard_processed['X_train'],\n",
    "        creditcard_processed['y_train'],\n",
    "        creditcard_processed['X_test'],\n",
    "        creditcard_processed['y_test'],\n",
    "        creditcard_processed['feature_names']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nBest model for credit card fraud: {cc_best_model}\")\n",
    "    display(cc_comparison[['f1_score', 'precision', 'recall', 'pr_auc', 'roc_auc']].round(4))\n",
    "    \n",
    "else:\n",
    "    print(\"Credit card dataset not available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KEY FINDINGS AND INSIGHTS ===\n",
      "\n",
      "1. MODEL PERFORMANCE SUMMARY\n",
      "----------------------------------------\n",
      "\n",
      "2. FRAUD DRIVER INSIGHTS (from SHAP analysis)\n",
      "--------------------------------------------------\n",
      "\n",
      "3. BUSINESS RECOMMENDATIONS\n",
      "------------------------------\n",
      "• Implement real-time scoring using the best performing model\n",
      "• Focus monitoring on high-risk features identified by SHAP\n",
      "• Set appropriate thresholds balancing fraud detection vs customer experience\n",
      "• Regular model retraining to adapt to new fraud patterns\n",
      "• Use SHAP explanations for fraud investigation and rule creation\n"
     ]
    }
   ],
   "source": [
    "# Summary of key findings\n",
    "print(\"=== KEY FINDINGS AND INSIGHTS ===\")\n",
    "print(\"\\n1. MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'fraud_comparison' in locals():\n",
    "    print(f\"\\nFraud Detection Dataset:\")\n",
    "    print(f\"• Best Model: {fraud_best_model}\")\n",
    "    print(f\"• F1-Score: {fraud_comparison.loc[fraud_best_model, 'f1_score']:.4f}\")\n",
    "    print(f\"• PR-AUC: {fraud_comparison.loc[fraud_best_model, 'pr_auc']:.4f}\")\n",
    "    print(f\"• Recall: {fraud_comparison.loc[fraud_best_model, 'recall']:.4f}\")\n",
    "\n",
    "if 'cc_comparison' in locals():\n",
    "    print(f\"\\nCredit Card Dataset:\")\n",
    "    print(f\"• Best Model: {cc_best_model}\")\n",
    "    print(f\"• F1-Score: {cc_comparison.loc[cc_best_model, 'f1_score']:.4f}\")\n",
    "    print(f\"• PR-AUC: {cc_comparison.loc[cc_best_model, 'pr_auc']:.4f}\")\n",
    "    print(f\"• Recall: {cc_comparison.loc[cc_best_model, 'recall']:.4f}\")\n",
    "\n",
    "print(\"\\n2. FRAUD DRIVER INSIGHTS (from SHAP analysis)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'fraud_insights' in locals() and fraud_insights:\n",
    "    if 'top_fraud_drivers' in fraud_insights:\n",
    "        print(\"\\nTop Risk Factors:\")\n",
    "        for driver in fraud_insights['top_fraud_drivers'][:5]:\n",
    "            print(f\"• {driver['feature']}: {driver['interpretation']}\")\n",
    "    \n",
    "    if 'protective_factors' in fraud_insights:\n",
    "        print(\"\\nProtective Factors:\")\n",
    "        for factor in fraud_insights['protective_factors'][:5]:\n",
    "            print(f\"• {factor['feature']}: {factor['interpretation']}\")\n",
    "\n",
    "print(\"\\n3. BUSINESS RECOMMENDATIONS\")\n",
    "print(\"-\" * 30)\n",
    "print(\"• Implement real-time scoring using the best performing model\")\n",
    "print(\"• Focus monitoring on high-risk features identified by SHAP\")\n",
    "print(\"• Set appropriate thresholds balancing fraud detection vs customer experience\")\n",
    "print(\"• Regular model retraining to adapt to new fraud patterns\")\n",
    "print(\"• Use SHAP explanations for fraud investigation and rule creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models and preprocessing objects saved for deployment!\n"
     ]
    }
   ],
   "source": [
    "# Save best models and preprocessing objects for deployment\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "if 'fraud_best_model' in locals():\n",
    "    # Save fraud detection model and preprocessing objects\n",
    "    joblib.dump(fraud_models[fraud_best_model], f'../models/fraud_best_model_{fraud_best_model}.pkl')\n",
    "    joblib.dump(fraud_processed['scaler'], '../models/fraud_scaler.pkl')\n",
    "    \n",
    "    # Save feature names\n",
    "    with open('../models/fraud_feature_names.txt', 'w') as f:\n",
    "        for feature in fraud_processed['feature_names']:\n",
    "            f.write(f\"{feature}\\n\")\n",
    "    \n",
    "    print(f\"Fraud detection model saved: {fraud_best_model}\")\n",
    "\n",
    "if 'cc_best_model' in locals():\n",
    "    # Save credit card model and preprocessing objects\n",
    "    joblib.dump(cc_models[cc_best_model], f'../models/creditcard_best_model_{cc_best_model}.pkl')\n",
    "    joblib.dump(creditcard_processed['scaler'], '../models/creditcard_scaler.pkl')\n",
    "    \n",
    "    # Save feature names\n",
    "    with open('../models/creditcard_feature_names.txt', 'w') as f:\n",
    "        for feature in creditcard_processed['feature_names']:\n",
    "            f.write(f\"{feature}\\n\")\n",
    "    \n",
    "    print(f\"Credit card model saved: {cc_best_model}\")\n",
    "\n",
    "print(\"\\nModels and preprocessing objects saved for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
